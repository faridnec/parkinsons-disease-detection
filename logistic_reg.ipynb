{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression in the Diagnosis of Parkinson's Disease\n",
    "In this notebook we will implement multiple regression to diagnose parkinson disease, the dataset is obtained from Oxford Parkinson's Disease Detection Dataset (https://archive.ics.uci.edu/dataset/174/parkinsons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools\n",
    "In this project these libraries are used: \n",
    "- NumPy, a popular library for scientific computing\n",
    "- Matplotlib, a popular library for plotting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import copy, math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "Background:\n",
    "- Parkinson's disease is a progressive neurodegenerative disorder that affects movement control. Early and accurate diagnosis is crucial for effective management and intervention. Machine learning techniques, such as logistic regression, offer a promising approach to diagnosing Parkinson's disease based on relevant clinical features.\n",
    "\n",
    "Objective:\n",
    "- The goal of this project is to implement logistic regression to develop a diagnostic model for Parkinson's disease. Leveraging a dataset containing clinical measurements and relevant features, the model will learn to distinguish between patients with Parkinson's disease and those without.\n",
    "\n",
    "Dataset:\n",
    "- The dataset consists of clinical measurements collected from individuals, including features related to voice and motor control. Each example in the dataset is labeled with the binary outcome: whether the individual has been diagnosed with Parkinson's disease (1) or not (0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin by looking at the structure of the csv that contains the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header looks like this:\n",
      "\n",
      "name,MDVP:Fo(Hz),MDVP:Fhi(Hz),MDVP:Flo(Hz),MDVP:Jitter(%),MDVP:Jitter(Abs),MDVP:RAP,MDVP:PPQ,Jitter:DDP,MDVP:Shimmer,MDVP:Shimmer(dB),Shimmer:APQ3,Shimmer:APQ5,MDVP:APQ,Shimmer:DDA,NHR,HNR,status,RPDE,DFA,spread1,spread2,D2,PPE\n",
      "\n",
      "First data point looks like this:\n",
      "\n",
      "phon_R01_S01_1,119.99200,157.30200,74.99700,0.00784,0.00007,0.00370,0.00554,0.01109,0.04374,0.42600,0.02182,0.03130,0.02971,0.06545,0.02211,21.03300,1,0.414783,0.815285,-4.813031,0.266482,2.301442,0.284654\n",
      "\n",
      "Second data point looks like this:\n",
      "\n",
      "phon_R01_S01_2,122.40000,148.65000,113.81900,0.00968,0.00008,0.00465,0.00696,0.01394,0.06134,0.62600,0.03134,0.04518,0.04368,0.09403,0.01929,19.08500,1,0.458359,0.819521,-4.075192,0.335590,2.486855,0.368674\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PARKINSONS_CSV = './data/PARKINSONS.csv'\n",
    "\n",
    "with open(PARKINSONS_CSV, 'r') as csvfile:\n",
    "    print(f\"Header looks like this:\\n\\n{csvfile.readline()}\")    \n",
    "    print(f\"First data point looks like this:\\n\\n{csvfile.readline()}\")\n",
    "    print(f\"Second data point looks like this:\\n\\n{csvfile.readline()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "### Information\n",
    "- This dataset is composed of a range of biomedical voice measurements from 31 people, 23 with Parkinson's disease (PD). Each column in the table is a particular voice measure, and each row corresponds one of 195 voice recording from these individuals (\"name\" column). The main aim of the data is to discriminate healthy people from those with PD, according to \"status\" column which is set to 0 for healthy and 1 for PD. \n",
    "\n",
    "- The data is in ASCII CSV format. The rows of the CSV file contain an instance corresponding to one voice recording. There are around six recordings per patient, the name of the patient is identified in the first column.For further information or to pass on comments, please contact Max Little (littlem '@' robots.ox.ac.uk).\n",
    "\n",
    "- Reference:\n",
    "\n",
    "  Max A. Little, Patrick E. McSharry, Eric J. Hunter, Lorraine O. Ramig (2008), 'Suitability of dysphonia measurements for telemonitoring of Parkinson's disease', IEEE Transactions on Biomedical Engineering\n",
    "\n",
    "### Load data\n",
    "- The `load_data()` function shown below loads the data into variables `X_train` and `y_train`\n",
    "  - `X_train` is a range of biomedical voice measurements from 31 people, 23 with Parkinson's disease (PD)\n",
    "  - `y_train` is whether or not the person has Parkinson's disease\n",
    "      - `y_train = 1` if the disease is diagnosed\n",
    "      - `y_train = 0` if the disease is not diagnosed\n",
    "  - Both `X_train` and `y_train` are numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View the variables and Dimension\n",
    "The code below prints the variable `x_train` and the type of the variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Shape: (195, 22), X Type:<class 'numpy.ndarray'>)\n",
      "Type of x_train: <class 'numpy.ndarray'>\n",
      "First three elements of x_train are:\n",
      " [[ 1.199920e+02  1.573020e+02  7.499700e+01  7.840000e-03  7.000000e-05\n",
      "   3.700000e-03  5.540000e-03  1.109000e-02  4.374000e-02  4.260000e-01\n",
      "   2.182000e-02  3.130000e-02  2.971000e-02  6.545000e-02  2.211000e-02\n",
      "   2.103300e+01  4.147830e-01  8.152850e-01 -4.813031e+00  2.664820e-01\n",
      "   2.301442e+00  2.846540e-01]\n",
      " [ 1.224000e+02  1.486500e+02  1.138190e+02  9.680000e-03  8.000000e-05\n",
      "   4.650000e-03  6.960000e-03  1.394000e-02  6.134000e-02  6.260000e-01\n",
      "   3.134000e-02  4.518000e-02  4.368000e-02  9.403000e-02  1.929000e-02\n",
      "   1.908500e+01  4.583590e-01  8.195210e-01 -4.075192e+00  3.355900e-01\n",
      "   2.486855e+00  3.686740e-01]\n",
      " [ 1.166820e+02  1.311110e+02  1.115550e+02  1.050000e-02  9.000000e-05\n",
      "   5.440000e-03  7.810000e-03  1.633000e-02  5.233000e-02  4.820000e-01\n",
      "   2.757000e-02  3.858000e-02  3.590000e-02  8.270000e-02  1.309000e-02\n",
      "   2.065100e+01  4.298950e-01  8.252880e-01 -4.443179e+00  3.111730e-01\n",
      "   2.342259e+00  3.326340e-01]]\n"
     ]
    }
   ],
   "source": [
    "# print x_train\n",
    "print(f\"X Shape: {X_train.shape}, X Type:{type(X_train)})\")\n",
    "print(\"Type of x_train:\",type(X_train))\n",
    "print(\"First three elements of x_train are:\\n\", X_train[:3]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y Shape: (195,), y Type:<class 'numpy.ndarray'>)\n",
      "Type of y_train: <class 'numpy.ndarray'>\n",
      "First five elements of y_train are:\n",
      " [1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# print y_train\n",
    "print(f\"y Shape: {y_train.shape}, y Type:{type(y_train)})\")\n",
    "print(\"Type of y_train:\",type(y_train))\n",
    "print(\"First five elements of y_train are:\\n\", y_train[:5])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Data\n",
    "\n",
    "Before starting to implement the learning algorithm, it is always good to visualize the data if possible.\n",
    "- The code below displays the data on a 2D plot (as shown below), where the axes are y label features.\n",
    "- A helper function in the ``utils.py`` file to generate this plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid function\n",
    "\n",
    "Logistic regression model is represented as\n",
    "\n",
    "$$f_{\\mathbf{w},b}(x) = g(\\mathbf{w}\\cdot \\mathbf{x} + b)$$\n",
    "where function $g$ is the sigmoid function. The sigmoid function is defined as:\n",
    "\n",
    "$$g(z) = \\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "Note that \n",
    "- `z` is not always a single number, but can also be an array of numbers. \n",
    "- If the input is an array of numbers, it is good to apply the sigmoid function to each value in the input array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Args:\n",
    "        z (ndarray): A scalar, numpy array of any size.\n",
    "\n",
    "    Returns:\n",
    "        g (ndarray): sigmoid(z), with the same shape as z\n",
    "         \n",
    "    \"\"\"\n",
    "    g = 1/(1 + np.exp(-z))\n",
    "    \n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For large positive values of x, the sigmoid should be close to 1, while for large negative values, the sigmoid should be close to 0. \n",
    "- Evaluating `sigmoid(0)` should give you exactly 0.5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid(0) = 0.5\n"
     ]
    }
   ],
   "source": [
    "#You can edit this value\n",
    "value = 0\n",
    "\n",
    "print (f\"sigmoid({value}) = {sigmoid(value)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2.4\"></a>\n",
    "### Cost function for logistic regression\n",
    "\n",
    "The `compute_cost` function using the equations below.\n",
    "\n",
    "Logistic regression cost function is of the form \n",
    "\n",
    "$$ J(\\mathbf{w},b) = \\frac{1}{m}\\sum_{i=0}^{m-1} \\left[ loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) \\right] \\tag{1}$$\n",
    "\n",
    "where\n",
    "* m is the number of training examples in the dataset\n",
    "\n",
    "\n",
    "* $loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)})$ is the cost for a single data point, which is - \n",
    "\n",
    "    $$loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = (-y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\tag{2}$$\n",
    "    \n",
    "    \n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ is the model's prediction, while $y^{(i)}$, which is the actual label\n",
    "\n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(\\mathbf{w} \\cdot \\mathbf{x^{(i)}} + b)$ where function $g$ is the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, w, b, *argv):\n",
    "    \"\"\"\n",
    "    Computes the cost over all examples\n",
    "    Args:\n",
    "      X : (ndarray Shape (m,n)) data, m examples by n features\n",
    "      y : (ndarray Shape (m,))  target value \n",
    "      w : (ndarray Shape (n,))  values of parameters of the model      \n",
    "      b : (scalar)              value of bias parameter of the model\n",
    "      *argv : unused, for compatibility with regularized version below\n",
    "    Returns:\n",
    "      total_cost : (scalar) cost \n",
    "    \"\"\"\n",
    "\n",
    "    m, n = X.shape\n",
    "    \n",
    "    cost = 0.\n",
    "    epsilon = 1e-15  # Small constant to avoid log(0) and division by zero\n",
    "    \n",
    "    for i in range(m):\n",
    "        z_i = np.dot(X[i], w) + b\n",
    "        f_wb_i = sigmoid(z_i)\n",
    "        # Avoiding extreme values to prevent NaN issues\n",
    "        f_wb_i = np.clip(f_wb_i, epsilon, 1 - epsilon)\n",
    "        cost += - y[i] * np.log(f_wb_i) - (1 - y[i]) * np.log(1 - f_wb_i)\n",
    "    \n",
    "    total_cost = cost / m\n",
    "\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell below is to check the implementation of the `compute_cost` function with two different initializations of the parameters $w$ and $b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at initial w and b (zeros): 0.693\n"
     ]
    }
   ],
   "source": [
    "m, n = X_train.shape\n",
    "\n",
    "# Compute and display cost with w and b initialized to zeros\n",
    "initial_w = np.zeros(n)\n",
    "initial_b = 0.\n",
    "cost = compute_cost(X_train, y_train, initial_w, initial_b)\n",
    "print('Cost at initial w and b (zeros): {:.3f}'.format(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient for logistic regression\n",
    "\n",
    "Gradient descent algorithm is:\n",
    "\n",
    "$$\\begin{align*}& \\text{repeat until convergence:} \\; \\lbrace \\newline \\; & b := b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b} \\newline       \\; & w_j := w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{1}  \\; & \\text{for j := 0..n-1}\\newline & \\rbrace\\end{align*}$$\n",
    "\n",
    "where, parameters $b$, $w_j$ are all updated simultaniously\n",
    "\n",
    "This `compute_gradient` function is to compute $\\frac{\\partial J(\\mathbf{w},b)}{\\partial w}$, $\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}$ from equations (2) and (3) below.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - \\mathbf{y}^{(i)}) \\tag{2}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - \\mathbf{y}^{(i)})x_{j}^{(i)} \\tag{3}\n",
    "$$\n",
    "* m is the number of training examples in the dataset\n",
    "\n",
    "    \n",
    "*  $f_{\\mathbf{w},b}(x^{(i)})$ is the model's prediction, while $y^{(i)}$ is the actual label\n",
    "\n",
    "\n",
    "- **Note**: While this gradient looks identical to the linear regression gradient, the formula is actually different because linear and logistic regression have different definitions of $f_{\\mathbf{w},b}(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, w, b, *argv): \n",
    "    \"\"\"\n",
    "    Computes the gradient for logistic regression \n",
    " \n",
    "    Args:\n",
    "      X : (ndarray Shape (m,n)) data, m examples by n features\n",
    "      y : (ndarray Shape (m,))  target value \n",
    "      w : (ndarray Shape (n,))  values of parameters of the model      \n",
    "      b : (scalar)              value of bias parameter of the model\n",
    "      *argv : unused, for compatibility with regularized version below\n",
    "    Returns\n",
    "      dj_dw : (ndarray Shape (n,)) The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db : (scalar)             The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    dj_dw = np.zeros(w.shape)\n",
    "    dj_db = 0.\n",
    "\n",
    "    for i in range(m):\n",
    "        z_wb = np.dot(X[i], w) + b\n",
    "        f_wb = sigmoid(z_wb)\n",
    "        \n",
    "        dj_db_i = f_wb - y[i]\n",
    "        dj_db += dj_db_i\n",
    "        \n",
    "        for j in range(n): \n",
    "            dj_dw[j] += X[i, j] * dj_db_i\n",
    "            \n",
    "    dj_db /= m\n",
    "    dj_dw /= m\n",
    "        \n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cells below is to check the implementation of the `compute_gradient` function with two different initializations of the parameters $w$ and $b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dj_db at initial w and b (zeros):-0.25384615384615383\n",
      "dj_dw at initial w and b (zeros):[-32.32963846153845, -43.50341282051283, -22.41898205128205, -0.002158589743589745, -1.6225641025641027e-05, -0.001179358974358975, -0.0012170769230769227, -0.003538179487179484, -0.010518512820512813, -0.10101282051282048, -0.005492743589743587, -0.006352410256410258, -0.008765717948717945, -0.016478102564102568, -0.009597025641025648, -4.868217948717946, -0.1403319230769231, -0.18779646153846152, 1.1783795717948717, -0.07379868205128204, -0.6605768641025636, -0.07299468717948715]\n"
     ]
    }
   ],
   "source": [
    "# Compute and display gradient with w and b initialized to zeros\n",
    "initial_w = np.zeros(n)\n",
    "initial_b = 0.\n",
    "\n",
    "dj_db, dj_dw = compute_gradient(X_train, y_train, initial_w, initial_b)\n",
    "print(f'dj_db at initial w and b (zeros):{dj_db}' )\n",
    "print(f'dj_dw at initial w and b (zeros):{dj_dw.tolist()}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning parameters using gradient descent \n",
    "\n",
    "Finding the optimal parameters of a logistic regression model by using gradient descent. \n",
    "\n",
    "- A good way to verify that gradient descent is working correctly is to look\n",
    "at the value of $J(\\mathbf{w},b)$ and check that it is decreasing with each step. \n",
    "\n",
    "- The value of $J(\\mathbf{w},b)$ should never increase, and should converge to a steady value by the end of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters, lambda_): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn theta. Updates theta by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      X :    (ndarray Shape (m, n) data, m examples by n features\n",
    "      y :    (ndarray Shape (m,))  target value \n",
    "      w_in : (ndarray Shape (n,))  Initial values of parameters of the model\n",
    "      b_in : (scalar)              Initial value of parameter of the model\n",
    "      cost_function :              function to compute cost\n",
    "      gradient_function :          function to compute gradient\n",
    "      alpha : (float)              Learning rate\n",
    "      num_iters : (int)            number of iterations to run gradient descent\n",
    "      lambda_ : (scalar, float)    regularization constant\n",
    "      \n",
    "    Returns:\n",
    "      w : (ndarray Shape (n,)) Updated values of parameters of the model after\n",
    "          running gradient descent\n",
    "      b : (scalar)                Updated value of parameter of the model after\n",
    "          running gradient descent\n",
    "    \"\"\"\n",
    "    \n",
    "    # number of training examples\n",
    "    m = len(X)\n",
    "    \n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    w_history = []\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_db, dj_dw = gradient_function(X, y, w_in, b_in, lambda_)   \n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w_in = w_in - alpha * dj_dw               \n",
    "        b_in = b_in - alpha * dj_db              \n",
    "       \n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            cost =  cost_function(X, y, w_in, b_in, lambda_)\n",
    "            J_history.append(cost)\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters/10) == 0 or i == (num_iters-1):\n",
    "            w_history.append(w_in)\n",
    "            print(f\"Iteration {i:4}: Cost {float(J_history[-1]):8.2f}   \")\n",
    "        \n",
    "    return w_in, b_in, J_history, w_history #return w and J,w history for graphing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here to see the gradient descent converging\n",
    "\n",
    "If your gradient descent is not converging, there are a few potential issues to investigate:\n",
    "\n",
    "Learning Rate (Alpha):\n",
    "- Experiment with different learning rates. Try reducing the learning rate and observe the effect on convergence.\n",
    "\n",
    "Feature Scaling:\n",
    "- Normalize or standardize your features to have a similar scale. This can often help gradient descent converge more efficiently.\n",
    "\n",
    "Check for Bugs in Cost Function or Gradient Calculation:\n",
    "- Double-check your cost function and gradient calculations. Print or log intermediate values during training to identify where the issue might be occurring.\n",
    "\n",
    "Check for Numerical Stability Issues:\n",
    "- Check for operations like taking the logarithm, exponentiation, or division that may lead to extreme values. Use functions like np.clip to prevent values from going out of a reasonable range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost     0.65   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1000: Cost     0.58   \n",
      "Iteration 2000: Cost     0.58   \n",
      "Iteration 3000: Cost     0.57   \n",
      "Iteration 4000: Cost     0.57   \n",
      "Iteration 5000: Cost     0.57   \n",
      "Iteration 6000: Cost     0.56   \n",
      "Iteration 7000: Cost     0.56   \n",
      "Iteration 8000: Cost     0.56   \n",
      "Iteration 9000: Cost     0.55   \n",
      "Iteration 9999: Cost     0.55   \n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "initial_w = np.random.randn(22) * 0.01\n",
    "initial_b = 0\n",
    "\n",
    "# Some gradient descent settings (can be tweaked/ changed)\n",
    "iterations = 10000 \n",
    "alpha = 0.0001\n",
    "\n",
    "w,b, J_history,_ = gradient_descent(X_train ,y_train, initial_w, initial_b, \n",
    "                                   compute_cost, compute_gradient, alpha, iterations, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the gradient descent converges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, w, b): \n",
    "    \"\"\"\n",
    "    Predict whether the label is 0 or 1 using learned logistic\n",
    "    regression parameters w\n",
    "    \n",
    "    Args:\n",
    "      X : (ndarray Shape (m,n)) data, m examples by n features\n",
    "      w : (ndarray Shape (n,))  values of parameters of the model      \n",
    "      b : (scalar)              value of bias parameter of the model\n",
    "\n",
    "    Returns:\n",
    "      p : (ndarray (m,)) The predictions for X using a threshold at 0.5\n",
    "    \"\"\"\n",
    "    # number of training examples\n",
    "    m, n = X.shape   \n",
    "    p = np.zeros(m)\n",
    "   \n",
    "    ### START CODE HERE ### \n",
    "    # Loop over each example\n",
    "    for i in range(m):   \n",
    "        z_wb = np.dot(X[i], w) + b  # Linear combination of features and weights\n",
    "        f_wb = sigmoid(z_wb)  # Sigmoid function to get probabilities\n",
    "        \n",
    "        # Apply the threshold (0.5) to determine the class (0 or 1)\n",
    "        p[i] = 1 if f_wb > 0.5 else 0\n",
    "        \n",
    "    ### END CODE HERE ### \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 76.410256\n"
     ]
    }
   ],
   "source": [
    "#Compute accuracy on our training set\n",
    "p = predict(X_train, w,b)\n",
    "print('Train Accuracy: %f'%(np.mean(p == y_train) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
